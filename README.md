Here is the list of ML study and portfolio  projects that I have uploaded (in progress):

Portfolio:
1. [Decision Trees and Random Forest Project.ipynb](https://github.com/IulliaK/Machine-Learning/blob/main/Decision%20Trees%20and%20Random%20Forest%20Project.ipynb)

   Results: we analysed real financial data to see whether or not our model would be able to correctly predict if the loan would be returned in full. Visualisation included some histograms, jointplot, and countplot. We also converted a categorical variable. First, we trained one decision tree; next, a random forest model. Randm Forest performed (as expected) significantly better, with a very low level of Type I error.

2. [Logistic Regression with Python Project](https://github.com/IulliaK/Machine-Learning/blob/main/Logistic%20Regression%20with%20Python%20Project)
   
   Results: I used Logistic Regression to predict the class of a passenger on the Titanic - Survived or Deceased. Missing age values were imputed by using the mean age for the passenger class. I got satisfactory results which could be further improved by applying other ML models or working on a "Name" feature.

Study projects:
1. [Small Logistic Regression Project.ipynb ](https://github.com/IulliaK/Machine-Learning/blob/main/Small%20Logistic%20Regression%20Project.ipynb) on a fake data 

   Results: received quite good results - confusion matrix shows that mostly all were classified correctly. I used histogram and pairplot to visualise data.

2. [Decision Trees and Random Forests in Python.ipynb](https://github.com/IulliaK/Machine-Learning/blob/main/Decision%20Trees%20and%20Random%20Forests%20in%20Python.ipynb)

   Result: in this notebook, we worked with a small kythosis dataset to compare the application of a single decision tree and a random forest. Because the data is highly imbalanced (most observations note the absence of the disease after the operation), the difference isn't striking but still we proved that Random Forect performs better (5 missclassified observations instead of 8 for a Decision Tree).

3. Support Vector Machines with Python.ipynb
   
   Result: in this work, I took a built-in dataset - breast_cancer, and applied SVM to classify a tumor as malignant or bening. SVM got very good results - about 0.93, which was further enhanced with GridSearch, up to 0.94. To decide which model to use - with or without GreedSearch, a domain knowledge is required.
   

